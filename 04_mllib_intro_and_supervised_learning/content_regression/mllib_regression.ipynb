{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Regression\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: September 23, 2024\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "Learning Spark: Machine Learning with MLlib\n",
    "\n",
    "*Details on linear regression*  \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to major regression models in MLlib using the DataFrame API\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Linear regression\n",
    "- VectorAssembler\n",
    "- RegressionEvaluator\n",
    "\n",
    "---\n",
    "\n",
    "### Introduction to Regression\n",
    "\n",
    "Earlier, we discussed the classification problem where the response variable is discrete\n",
    "\n",
    "Regression is another common form of supervised learning  \n",
    "The response variable in a regression problem is quantitative or continuous  \n",
    "\n",
    "Several classification models also have regression counterparts, including:\n",
    "\n",
    "- Support vector machines  \n",
    "- Tree-based methods like random forests and gradient-boosted trees  \n",
    "\n",
    "**The distributed processing used by Spark is particularly helpful in random forests.**  \n",
    "The trees can be distributed across executors and built, and the results can be aggregated.\n",
    "\n",
    "To implement the regression counterpart, the same package is loaded but a different method is called.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Linear regression is the most fundamental model used in regression.\n",
    "\n",
    "Model assumes a linear relationship between a set of explanatory variables $X$ (aka features, factors, predictors, independent variables) and a scalar response variable $Y$.\n",
    "\n",
    "Linear regression models are most often fit using the *ordinary least squares* (*OLS*) approach.  \n",
    "\n",
    "A regularization term is often added to the loss function to help with generalization. Examples include:\n",
    "\n",
    "- ridge regression ($L^2$-norm penalty)\n",
    "- lasso ($L^1$-norm penalty)\n",
    "- elastic net is a blend of ridge and lasso regression\n",
    "\n",
    "More about this below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"mllib_classifier\").getOrCreate()\n",
    "\n",
    "# Load training data\n",
    "filename = \"sample_housing_data.csv\"\n",
    "training = spark.read.csv(filename,  inferSchema=True, header = True)\n",
    "training.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DataFrame API version of MLlib, features need to be assembled into a feature column for the ML \n",
    "Model\n",
    "\n",
    "`VectorAssembler` will handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# inputCols take a list of column names\n",
    "# outputCol is arbitrary name of new column; generally called features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"median_income\", \"total_rooms\"],\n",
    "                            outputCol=\"features\")\n",
    "\n",
    "tr = assembler.transform(training)\n",
    "tr.select(\"*\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the Linear Regression Model and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol='features',         # feature vector name\n",
    "                      labelCol='median_house_value',  # target variable name\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3, \n",
    "                      elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(tr)\n",
    "\n",
    "# Print the weights and intercept for linear regression\n",
    "print(\"Weights: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's measure the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# compute predictions. this will append column \"prediction\" to dataframe\n",
    "lrPred = lrModel.transform(tr)\n",
    "lrPred.show(1)\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"median_house_value\")\n",
    "\n",
    "print('-'*20)\n",
    "print(\"METRICS\")\n",
    "print(\"Mean Squared Error:\", ev.evaluate(lrPred, {ev.metricName: \"mse\"}))\n",
    "print(\"R Squared:\", ev.evaluate(lrPred, {ev.metricName:'r2'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "Notice we extracted a metric like MSE by: \n",
    "1. passing to the evaluator a dataframe with labels and predictions\n",
    "2. going to the Regression Evaluator dictionary with the desired key: \"mse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ev.evaluate(lrPred, {ev.metricName: \"mse\"})```\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization Parameters\n",
    "\n",
    "You might have noticed the parameters `regParam` and `elasticNetParam` in the function call above.  \n",
    "\n",
    "The regularization term has this form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"elastic_net_term.png\" width=200>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ is `elasticNetParam` parameter that mixes Ridge, Lasso penalties, respectively \n",
    "\n",
    "The parameter $\\lambda$ will weigh prediction error penalty against regularization penalty \n",
    "\n",
    "The regularization term often helps a model better generalize to new data by reducing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Regression Models using the DataFrame API\n",
    "\n",
    "PySpark supports several regression models including:  \n",
    "- Generalized linear regression\n",
    "- **Decision tree regression** - implementation partitions data by rows, allowing distributed training with millions or even billions of instances.\n",
    "\n",
    "- Random forest regression\n",
    "- Gradient-boosted tree regression\n",
    "\n",
    "For more details, including code examples, please see [here](https://spark.apache.org/docs/latest/ml-classification-regression.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Copy the Linear Regression example from above, and modify in the cells below to fit and evaluate these two models:  \n",
    "\n",
    "i. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Think of at least one real-world example of when you would need to implement each of the following tasks:  \n",
    "- regression\n",
    "- binary classification\n",
    "- multiclass classification\n",
    "- multilabel classification\n",
    "\n",
    "If you are not sure about the difference between multiclass and multilabel, here is one resource:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/multiclass.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS7200 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
