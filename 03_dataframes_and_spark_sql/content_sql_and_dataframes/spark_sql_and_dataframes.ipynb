{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"uva_seal.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL and DataFrames\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: September 6, 2023\n",
    "\n",
    "---  \n",
    "\n",
    "### Sources \n",
    "\n",
    "Learning Spark, Chapter 9: Spark SQL\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/apache-spark-tutorial-machine-learning\n",
    "\n",
    "Demonstration of several useful DataFrame operations:  \n",
    "https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to Spark SQL, the interface for working with structured and semistructured data\n",
    "- Introduce DataFrames and show basic functionality\n",
    "- Explain the benefits of the Parquet format\n",
    "\n",
    "### CONCEPTS AND FUNCTIONS\n",
    "- Schema\n",
    "- SQL\n",
    "- Dataset and DataFrame\n",
    "- Partition\n",
    "- Parquet files\n",
    "\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "\n",
    "Two important ways of working with big data in Spark: \n",
    "\n",
    "- through Spark SQL\n",
    "- using DataFrames\n",
    "\n",
    "They also interoperate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Schema in Spark\n",
    "\n",
    "The schema in Spark defines the data structure. For each field, a 3-tuple is specified: `(column name, data type, nullable)`  \n",
    "\n",
    "---  \n",
    "\n",
    "**Example of schema with two Fields *author* and *pages*, which cannot contain null values**\n",
    "```\n",
    "schema = StructType([StructField(\"author\", StringType(), False), StructField(\"pages\", IntegerType(), False)])\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to allow Spark to infer the schema of your data, but it's preferable to feed it the schema:\n",
    "\n",
    "- avoids having Spark launch a separate job to read a large fraction of the data to infer schema\n",
    "- early detection of errors if the data doesn't match the schema\n",
    "- Spark inference may be incorrect. For example, it may think all numerical data are strings.\n",
    "\n",
    "### This schema is different from database schema\n",
    "\n",
    "A database *schema* is the structure that represents the logical view of the entire database.  \n",
    "It defines how data is organized and how relations among them are associated.  \n",
    "This is implemented through the use of tables, views, and integrity constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./db_schema.png\" width=500> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Common Spark Data Types\n",
    "\n",
    "- integer types, all `int` in python:\n",
    "  - ShortType\n",
    "  - IntegerType\n",
    "  - LongType\n",
    "  - FloatType\n",
    "  - DoubleType\n",
    "- StringType\n",
    "- BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. SQL in Ten Seconds (tongue in cheek)\n",
    "\n",
    "\n",
    "SQL is a structured query language used to communicate with relational databases.  \n",
    "Commands include CREATE, SELECT, UPDATE, ALTER, INSERT INTO, DROP, DELETE.  \n",
    "This course will focus on SELECT.\n",
    "\n",
    "### 5. Spark SQL Capabilities:\n",
    "\n",
    "- load data from various structured formats including JSON, Hive, Parquet  \n",
    "- query data using SQL inside Spark or from external tools that connect to Spark (e.g., `Tableau`) \n",
    "- Spark SQL integrates between SQL and Python/Java/Scala/R code. Can do things like join RDDs and SQL tables.\n",
    "\n",
    "### 6. Note on Spark SQL Development\n",
    "\n",
    " Spark SQL has been heavy development area in new releases.  \n",
    " As the module involves massive amounts of data, optimizing operations is valuable.  \n",
    " We will discuss the optimizer behind Spark SQL.\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Dataset and DataFrame\n",
    "\n",
    "- A Dataset is a distributed collection of data   \n",
    "- A Dataset can be constructed from JVM objects and then manipulated using functional transformations (`map()`, `flatMap()`, `filter()`, etc.)  \n",
    "- A DataFrame is a Dataset organized into named columns   \n",
    "\n",
    "In practice, you will be thinking in terms of `DataFrames`, and not `Datasets`.  For users familiar with dataframes from R and Python, they are similar, yet with operations distinct to Spark.  As an example, adding a new column to a DataFrame is executed using `withColumn()`.  This may feel more formal compared to R and Python.  \n",
    "\n",
    "Additionally - when compared to R and Python - **the Spark DataFrame uses richer optimizations under the hood.  \n",
    "The structure makes use of distributed computing, in the same manner as RDDs.**  \n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.  \n",
    "\n",
    "The DataFrame API is available in Scala, Java, Python, and R. \n",
    "\n",
    "### 8. DataFrames vs RDDs  \n",
    "\n",
    "Now that we have two powerful objects that parallelize data, we have more flexibility, but this can lead to confusion.  When is it better to use DataFrames, and when is it better to use RDDs?  \n",
    "\n",
    "Here are some recommendations:   \n",
    "\n",
    "- In general, most work can be done with DataFrames  \n",
    "\n",
    "- Use DataFrames to use high-level expressions, to perform SQL queries to explore the data, and to gain columnar access.  For example, if you are thinking about the data by field names, you probably want the data in a DataFrame.\n",
    "\n",
    "- For machine learning and building predictive models, DataFrames are recommended. You will be exploring the data by column, and building features from the columns of data.  \n",
    "- RDDs can be useful to perform low-level transformations and actions on unstructured data. For example, filtering strings and performing other simple transformations on text is best done with RDDs.  In these cases, the analyst doesn't care about field names, and there is no need to impose schema on the data.  \n",
    "\n",
    "- Use RDDs when you want to manipulate the data with functional programming constructs rather than domain specific expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Creating a DataFrame\n",
    "\n",
    "There are multiple ways to do this:\n",
    "- use a function such as `read.csv()` to read data from files into DataFrames (most common)\n",
    "- pass data to `createDataFrame()` with schema\n",
    "- conversion from RDD using `toDF()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: Create DataFrame from RDD using `toDF()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "```\n",
    "# import modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "\n",
    "df = rdd.map(lambda line: Row(longitude=line[0], \n",
    "                              latitude=line[1], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[7],\n",
    "                              medianHouseValue=line[8])).toDF()\n",
    "```\n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Create DataFrame by passing data and schema to `createDataFrame()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# set up the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# create some data; list of tuples\n",
    "data = [\n",
    "    (0, \"ChatGPT is all the rage\"),\n",
    "    (1, \"Google released BARD to compete\"),\n",
    "    (2, \"What does AWS think about this?\")\n",
    "]\n",
    "\n",
    "# define schema; each field holds (name, data type, nullable)\n",
    "# for large number of fields, best to automate schema construction\n",
    "schema = StructType([StructField('id', IntegerType(), False), \n",
    "                     StructField('sentence', StringType(), False)])\n",
    "\n",
    "# create df by passing data, schema\n",
    "sentenceDataFrame = spark.createDataFrame(data, schema)\n",
    "\n",
    "# print first few records\n",
    "sentenceDataFrame.show(3, False)\n",
    "\n",
    "# print data type\n",
    "print(type(sentenceDataFrame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3: Create a DataFrame from some JSON data with spark.read()**  \n",
    "(For an example of JSON data see: http://json.org/example.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read data in json format\n",
    "df = spark.read.json(\"people.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an RDD from a DataFrame**\n",
    "\n",
    "This is very simple: `df.rdd`\n",
    "\n",
    "Here we convert our df containing sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_rdd = sentenceDataFrame.rdd\n",
    "print(sentence_rdd.take(2))\n",
    "print(type(sentence_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Some Useful DataFrames Operations\n",
    "\n",
    "Next we explore subsetting, filtering, and aggregation among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data in json format\n",
    "df = spark.read.json(\"people.json\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "There are three ways to select columns and we show them all. Find your favorite.\n",
    "\n",
    "- using bracket operator\n",
    "- using `col()` method\n",
    "- using dot operator (my favorite)\n",
    "\n",
    "We see them below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records where age > 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bracket operator\n",
    "df.filter(df['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot operator\n",
    "df.filter(df.age > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column operator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col('age') > 21).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep records subject to filters on name, then sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "# alternatively using df.name instead of col(\"name\")\n",
    "df.filter((df.name == \"Andy\") | (df.name == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Fetch records with age *null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"age\").isNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch records with age *not null*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(col(\"age\").isNotNull()).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where() is equivalent to filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where((df.name == \"Andy\") | (df.name == \"Michael\")).sort(asc(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing with 0 (just for illustration; not a great idea for this data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the age field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Spark SQL Queries and Temp Views\n",
    "\n",
    "Spark SQL is at the interface of SQL and DataFrames.    \n",
    "To write SQL queries against DataFrames, first register DF as a `SQL temp view`, and then write the query.\n",
    "\n",
    "A temp view is session scoped:  \n",
    "- visible only to the session that created it \n",
    "- dropped when the session ends (not persisted)\n",
    "\n",
    "**Example of SQL Query against DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register DataFrame as temp view with name \"people\"\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# query the view\n",
    "sqlDF = spark.sql(\"SELECT * FROM people where name == 'Andy'\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Aggregate on columns\n",
    "\n",
    "SQL functions can be loaded from this library: `pyspark.sql.functions`\n",
    "\n",
    "We will load some stock data to demonstrate aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data types and functions\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType, LongType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# set up schema\n",
    "stock_schema = StructType([StructField('date',DateType(),False),\n",
    "                           StructField('ticker',StringType(),False),\n",
    "                           StructField('close',FloatType(),False),\n",
    "                           StructField('adjusted_close',FloatType(),False),\n",
    "                           StructField('volume',LongType(),False),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in stock data. Source: Yahoo! finance\n",
    "DATAPATH_STOCKS = './amzn_msft_prices.csv'\n",
    "\n",
    "df_stx = spark.read.csv(DATAPATH_STOCKS, header=True, schema=stock_schema)\n",
    "df_stx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the schema\n",
    "\n",
    "df_stx.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will calculate some statistics on each stock:\n",
    "- minimum closing price\n",
    "- maximum closing price\n",
    "- minimum volume\n",
    "- maximum volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df_stx.groupBy(\"ticker\").agg(F.min(\"close\"), F.max(\"close\"), F.min(\"volume\"), F.max(\"volume\"))\n",
    "agg_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**  \n",
    "Do NOT use loops to aggregate data. Loops are run sequentially and do not harness parallelization.  \n",
    "Using the `groupBy()` method will do the job using parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Input/Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13A. I/O with DataFrames\n",
    "\n",
    "Here we show various examples of reading and writing dataframes.  \n",
    "This code is for illustration only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in Data and Infer the Schema**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "```\n",
    "adult_df = spark.read.\\\n",
    "    format(\"com.spark.csv\").\\\n",
    "    option(\"header\", \"false\").\\\n",
    "    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and Write using Generic Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read and Write using Manually Specified Formats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "df = spark.read.load(\"examples/src/main/resources/people.json\", format=\"json\")\n",
    "df.select(\"name\", \"age\").write.save(\"namesAndAges.parquet\", format=\"parquet\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13B. Parquet Files\n",
    "\n",
    "- Parquet is a **columnar format** that is supported by many other data processing systems  \n",
    "\n",
    "- Stores metadata about the columns, which can provide efficiency\n",
    "\n",
    "- Files hold binary data\n",
    "\n",
    "- Read and writing parquet files can be MUCH faster in Spark\n",
    "\n",
    "- Especially useful when querying columns for analytics and ML (don't generally need entire rows of data)\n",
    "\n",
    "- Good compression and encoding options:  \n",
    "  Implements a hybrid of bit packing and RLE - encoding switches based on which produces the best compression results\n",
    "\n",
    "  *Bit Packing* - Usually 32 or 64 bits of storage dedicated per integer.  \n",
    "  For small integers, packs multiple integers into same space for efficient storage.  \n",
    "  \n",
    "  *Run Length Encoding* - for sequence of duplicate values, store single value with number of occurrences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./RLE.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13C. Partition Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database tables can be partitioned to make querying more efficient.  \n",
    "For example, the data can be\n",
    "split by gender and country, producing smaller tables.  \n",
    "If the analyst is only interested in a single country, the query will run faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a partitioned table, data are usually stored in different directories, with partitioning column values encoded in the path of each partition directory.  \n",
    "\n",
    "All built-in file sources (including Text/CSV/JSON/ORC/Parquet) are able to discover and infer partitioning information automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path\n",
    "└── to\n",
    "    └── table\n",
    "        ├── gender=male\n",
    "        │   ├── ...\n",
    "        │   │\n",
    "        │   ├── country=US\n",
    "        │   │   └── data.parquet\n",
    "        │   ├── country=CN\n",
    "        │   │   └── data.parquet\n",
    "        │   └── ...\n",
    "        └── gender=female\n",
    "            ├── ...\n",
    "            │\n",
    "            ├── country=US\n",
    "            │   └── data.parquet\n",
    "            ├── country=CN\n",
    "            │   └── data.parquet\n",
    "            └── ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of writing DF to Parquet file, partitioning columns**\n",
    "\n",
    "```\n",
    "df = df.withColumn('end_month', F.month('end_date'))\n",
    "df = df.withColumn('end_year', F.year('end_date'))\n",
    "df.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Given the stock dataframe, use Spark SQL to select all AMZN records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "# register stock dataFrame as temp view with name \"stocks\"\n",
    "df_stx.createOrReplaceTempView(\"stocks\")\n",
    "\n",
    "# query the view\n",
    "sqlDF = spark.sql(\"SELECT * FROM stocks where ticker == 'AMZN'\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Given the stock dataframe, do an aggregation to compute minimum, mean, and maximum adjusted close for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_stx.groupBy(\"ticker\").agg(F.min(\"adjusted_close\"), F.mean(\"adjusted_close\"), F.max(\"adjusted_close\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Select the date, ticker, and adjusted_close columns, saving this data as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_stx.select(\"date\",\"ticker\",\"adjusted_close\").write.save(\"stocks.parquet\", format=\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Load the parquet file into a new dataframe and verify that things look correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test = spark.read.load(\"stocks.parquet\")\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Summary\n",
    "\n",
    "You should now have a basic understanding of Spark SQL, DataFrames, and how to use some of the common transformations on DataFrames.  \n",
    "With practice, you will gain comfort in selecting and processing data with Spark SQL and DataFrames, which is essential.\n",
    "\n",
    "Additionally, you should have some sense of when DataFrames are preferred over RDDs, and vice versa.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
