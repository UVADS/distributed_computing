{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipelines\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: October 8, 2024\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES\n",
    "\n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "- https://spark.apache.org/docs/latest/ml-pipeline.html  \n",
    "- http://blog.insightdatalabs.com/spark-pipelines-elegant-yet-powerful/  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "- Introduction to ML Pipelines  \n",
    "\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- ML Pipeline\n",
    "- `DataFrame`\n",
    "- `Transformer`\n",
    "- `Estimator`\n",
    "- `Parameter`\n",
    "\n",
    "---\n",
    "\n",
    "**ML Pipelines**\n",
    "\n",
    "ML Pipelines use the following objects:\n",
    "\n",
    "**Transformer**  \n",
    "Transforms one DataFrame into another DataFrame\n",
    "\n",
    "**Estimator**  \n",
    "An algorithm that can be fit on a DataFrame (e.g., Logistic Regression)\n",
    "\n",
    "**Parameter**  \n",
    "Properties of an estimator (e.g., max number of iterations, regularization parameter)\n",
    "\n",
    "*Setter methods* are available for setting parameters:\n",
    "\n",
    "**Set Parameters for Logistic Regression Instance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# lr is our logistic regression model\n",
    "\n",
    "lr.setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline**  \n",
    "A sequential chain of multiple `Transformers` and `Estimators` to specify an ML workflow  \n",
    "\n",
    "The pipeline in Spark is very similar to the pipeline in scikit-learn.  \n",
    "It acts as a workflow to keep all steps together from start to finish, for example:\n",
    "- Data preprocessing\n",
    "- Feature extraction\n",
    "- Model fitting\n",
    "- Model tuning\n",
    "\n",
    "Keeping track of these steps manually can be painful and error-prone.  \n",
    "For example, the analyst might train on the test set by accident.  That would be VERY bad.  \n",
    "\n",
    "Pipelines can be saved, loaded, and applied to any dataset containing the necessary columns of data.  This works in training mode and scoring mode (scoring mode is when we make predictions; it is also called *inference*).\n",
    "\n",
    "By encapsulating all of the steps in a pipeline, the required code for scoring becomes substantially less.  There is no need to code the steps again.  Simply load the pipeline, pass the data to it, and make predictions.  Data engineers love this!\n",
    "\n",
    "---\n",
    "\n",
    "**Pipeline Schematic**  \n",
    "`Cylinders` are DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_pipeline_graph.png\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA OUTLINE\n",
    "#train_df  dataframe containing labels (1=like, 0=dislike), restaurant reviews (string), ratings (integer) \n",
    "#             will be used to train LogReg model\n",
    "#test_df   dataframe with the same fields, set aside for model evaluation\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark= SparkSession.builder.getOrCreate()\n",
    "\n",
    "# some training data\n",
    "train_df = spark.createDataFrame([\n",
    "    (0, \"The food was terrible...and such small portions!\", 1),\n",
    "    (1, \"I would eat here EVERY DAY\", 5),\n",
    "    (1, \"LOVE LOVE LOVE the tacos!!\", 5)\n",
    "], [\"label\", \"review\", \"rating\"])\n",
    "\n",
    "train_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pipeline stages\n",
    "\n",
    "from pyspark.ml import Pipeline  \n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# process review data into first feature\n",
    "tok = Tokenizer(inputCol=\"review\", outputCol=\"words\")  \n",
    "htf = HashingTF(inputCol=\"words\", outputCol=\"tf\", numFeatures=200)  \n",
    "\n",
    "# process rating data into second feature\n",
    "ohe = OneHotEncoder(inputCol=\"rating\", outputCol=\"rc\") \n",
    "\n",
    "va = VectorAssembler(inputCols=[\"tf\",\"rc\"], outputCol=\"features\")  \n",
    "lr = LogisticRegression(labelCol='label', featuresCol='features', maxIter=10, regParam=0.01)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline = Pipeline(stages=[tok, htf, ohe, va, lr])\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test set\n",
    "\n",
    "test_df = spark.createDataFrame([\n",
    "    (0, \"I would give this place ZERO STARS if I could!\", 1),\n",
    "    (1, \"Yum!\", 5),\n",
    "    (1, \"Omg the best fries\", 5)\n",
    "], [\"label\", \"review\", \"rating\"])\n",
    "\n",
    "test_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "prediction = model.transform(test_df)\n",
    "prediction.select('label', 'rawPrediction','probability','prediction').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model gets the first instance wrong, but bear in mind it's a tiny training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, the pipeline outlines the steps that will take place sequentially: \n",
    "\n",
    "1. The data is processed into features  \n",
    "2. The features are combined using `VectorAssembler`  \n",
    "3. The combined features are input to the Logistic Regression model  \n",
    "\n",
    "Calling `pipeline.fit(train_df)` will actually execute the workflow  \n",
    "\n",
    "Each step is either a `Transformer` or an `Estimator`  \n",
    "\n",
    "Each of the preprocessing steps is a `Transformer`  \n",
    "The logistic regression is an `Estimator`  \n",
    "\n",
    "**Another Pipeline Example:**  \n",
    "https://spark.apache.org/docs/1.6.0/ml-guide.html  \n",
    "\n",
    "**Custom Transformers**  \n",
    "There are many transformers available in `MLlib`  \n",
    "Users can also create custom transformers.  \n",
    "\n",
    "`Transformer` requirements:  \n",
    "\n",
    "1. Implement the `transform` method  \n",
    "2. Specify an `inputCol` and `outputCol`  \n",
    "3. Accept a DataFrame as input and return a DataFrame as output  \n",
    "\n",
    "\n",
    "**Saving and Loading Pipeline**  \n",
    "As mentioned earlier, pipelines can be saved for future use.  \n",
    "This is helpful in several circumstances, including:  \n",
    "\n",
    "1. The user wishes to return to model development at a later time  \n",
    "2. Calling the pipeline to score records in production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **Running the Pipeline**  \n",
    "i. Copy the pipeline code to the cell below  \n",
    "ii. Pass some data to the pipeline  \n",
    "iii. Run the pipeline and show the predictions  \n",
    "iv. Measure the accuracy on the passed data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
