{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Training\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: October 17, 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SOURCES: \n",
    "\n",
    "- [Distributed Machine Learning Frameworks and its Benefits](https://www.xenonstack.com/blog/distributed-ml-framework#:~:text=In%20distributed%20machine%20learning%2C%20model,and%20training%20each%20split%20separately.)\n",
    "\n",
    "- [Distributed Training with Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training?view=azureml-api-2/)\n",
    "\n",
    "- [Distributed Training: Guide for Data Scientists\n",
    "](https://neptune.ai/blog/distributed-training)\n",
    "\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin.\n",
    "\n",
    "- [Distributed model training II: Parameter Server and AllReduce](http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/)\n",
    "\n",
    "- [Distributed Machine Learning and the Parameter Server](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture22.pdf)\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "\n",
    "- Explain approaches for distributed model training\n",
    "- Identify the benefits and challenges of synchronous and asynchronous training\n",
    "- Explain the parameter server and Allreduce algorithms\n",
    "- Explain why Ring-Allreduce can work better than Allreduce \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Data parallelism and model parallelism\n",
    "- Asynchronous training vs Synchronous training \n",
    "- Parameter server algorithm\n",
    "- Allreduce algorithm and Ring-Allreduce algorithm\n",
    "- Vertical partitioning and horizontal partitioning of a model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Why use Distributed Model Training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For massive training sets, it may not be possible to train a model on a single machine. \n",
    "\n",
    "This may be the case for deep learning models.\n",
    "\n",
    "In *distributed training*, the workload to train a model is split up and shared among worker nodes. \n",
    "\n",
    "The concepts, benefits, and challenges are similar to what we've learned earlier.\n",
    "\n",
    "The work can be parallelized to speed up training.\n",
    "\n",
    "This process introduces benefits but also complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Parallelism and Model Parallelism\n",
    "\n",
    "Two main types of distributed training: *data parallelism* and *model parallelism*.\n",
    "\n",
    "#### Data Parallelism\n",
    "\n",
    "This follows the approach used by Spark\n",
    "\n",
    "Data is divided into partitions\n",
    "\n",
    "Number of partitions = total number of available nodes\n",
    "\n",
    "Model is copied in each worker node\n",
    "\n",
    "Each node operates on its subset of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/data_parallelism.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node must:\n",
    "\n",
    "- Independently compute errors between training sample predictions and labels\n",
    "- Update its model based on errors\n",
    "- Communicate all of its changes to the other nodes to update their corresponding models\n",
    "\n",
    "Worker nodes need to synchronize gradients at end of batch computation to ensure they're training a consistent model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parallelism\n",
    "\n",
    "In some cases, the model may be too large for a worker\n",
    "\n",
    "The strategy is to segment the model into different parts that run concurrently in different workers\n",
    "\n",
    "Each model part runs on same data\n",
    "\n",
    "Scalability depends on degree of task parallelization of algorithm\n",
    "\n",
    "Worker nodes need to synchronize shared parameters, usually once for each forward or backward-propagation step \n",
    "\n",
    "More complex to implement than data parallelism\n",
    "\n",
    "**Example of model parallelized on two GPUs**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/model_parallelism.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model may be divided horizontally or vertically into different parts \n",
    "\n",
    "The parts run concurrently in different workers with each worker running on the same data\n",
    "\n",
    "Worker needs to synchronize shared parameters, usually once for each forward or backward-propagation step\n",
    "\n",
    "This **illustration** shows different partitions. Vertical partitioning keeps together all neurons in a layer, which works well for training deep learning models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/horiz_vert_part.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### III. Synchronous Training\n",
    "\n",
    "Consider data parallelism case: \n",
    "- Data is divided into partitions\n",
    "- Each partition is sent to a worker \n",
    "- Each worker has full replica of model. Training is done on its partition. \n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "In synchronous training, forward pass begins at same time for each worker\n",
    "\n",
    "Each worker computes different output\n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "Each worker performs backward pass and has local gradient (since trained on different data subsets):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/loss.png\" width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To synchronize across workers, gradients are aggregated (averaging is most common):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/avg_grad.png\" width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model parameters are updated using averaged gradient:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/gradient_descent.png\" width=150>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures all workers have synched parameters\n",
    "\n",
    "But how to efficiently aggregate?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Background: Reduce vs Allreduce\n",
    "\n",
    "We discussed **reduce**, as in MapReduce:\n",
    "\n",
    "Compute a reduction (generally a sum) of data on multiple machines $M_1, M_2, ..., M_n$ and materialize result on one machine $D$\n",
    "\n",
    "The difference with **Allreduce** is the destination of the result: it is materialized on the machines $M_1, M_2, ..., M_n$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Synchronous Algorithm Example: Allreduce Algorithm\n",
    "\n",
    "Each node has a subset of the data \n",
    "\n",
    "Each node does a calculation (e.g., model gradients) and distributes to the other nodes\n",
    "\n",
    "**Very communication heavy**\n",
    "\n",
    "Algorithm reduces the target arrays in all workers to a single array and returns the resultant array to all workers\n",
    "\n",
    "Gradients are collected across workers, averaged, used to update model weights on each node\n",
    "\n",
    "Operation proceeds elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Illustration where values are aggregated elementwise*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/allreduce.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are different implementations:\n",
    "\n",
    "**1. Naive Allreduce**\n",
    "\n",
    "Each worker sends gradients to single worker called the *driver*.  \n",
    "\n",
    "The driver reduces the gradients and sends updated gradients to all workers.  \n",
    "\n",
    "**Challenge with this approach**: the driver is a bottleneck.  \n",
    "Each node must ship gradients to the driver, and the driver must send updated gradients back to each node.  \n",
    "This scales linearly with $N$.  \n",
    "As the number of nodes increases, communication cost and reduction step scales poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/single_gpu_reducer.png\" width=600>\n",
    "\n",
    "Source [here](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/#:~:text=The%20GPUs%20in%20a%20ring,data%20from%20its%20left%20neighbor.&text=The%20algorithm%20proceeds%20in%20two,%2C%20and%20then%2C%20an%20allgather.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**2. Ring-Allreduce** \n",
    "\n",
    "This is a very clever approach that is strategic about the data structure and algorithm.\n",
    "\n",
    "Results in an algorithm that runs in **constant time.**\n",
    "\n",
    "The intuition is that each GPU does a fraction of the work and passes the result to its right neighbor\n",
    "\n",
    "It also receives a fraction of work from its left neighbor\n",
    "\n",
    "Workers are set up in a logical ring\n",
    "\n",
    "---\n",
    "\n",
    "*Benefits in Neural Networks*\n",
    "\n",
    "Each worker is in charge of subset of parameters (weights) shared only with the next worker in the ring  \n",
    "\n",
    "Less time spent sending data and more time spent doing computations on data locally on each GPU  \n",
    "\n",
    "Allows for efficient averaging of gradients in neural networks across many devices and many nodes  \n",
    "\n",
    "Retains the determinism and predictable convergence properties of synchronous stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/ringreduce.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How it Works**\n",
    "\n",
    "The algorithm consists of two steps:\n",
    "\n",
    "- Scatter-reduce: GPUs exchange data such that each GPU ends up with chunk of final result.\n",
    "- Allgather: GPUs exchange chunks such that all GPUs end up with complete final result.\n",
    "\n",
    "**For an elementwise addition task, we start with this:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/start.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We end with this:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/end.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we demonstrate *scatter-reduce*\n",
    "\n",
    "On each iteration, each GPU sends one chunk to neighbor on right and receives one chunk from neighbor on left\n",
    "\n",
    "The data is accumulated on each node and pushed around the ring like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/iteration1.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/iteration2.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After $N-1$ iterations, each node will hold the sum of all values across GPUs for a chunk (see below).\n",
    "\n",
    "Each worker does some of the reduction which eases workload\n",
    "\n",
    "This [post](https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/#:~:text=The%20GPUs%20in%20a%20ring,data%20from%20its%20left%20neighbor.&text=The%20algorithm%20proceeds%20in%20two,%2C%20and%20then%2C%20an%20allgather.) has a nice illustration and helpful details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/scatter_reduce_final.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Allgather*\n",
    "\n",
    "This step is very similar to scatter-reduce and runs for $N-1$ iterations\n",
    "\n",
    "Instead of accumulating values, the final sum is copied to the neighbor and overwrites its partial sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/allgather.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Communication Cost*\n",
    "\n",
    "Each of $N$ GPUs will send and receive values $N-1$ times for scatter-reduce, and $N-1$ times for allgather. \n",
    "\n",
    "On each iteration, GPUs send $\\frac{K}{N}$ values, where $K$ = number of values in array being summed across different GPUs. \n",
    "\n",
    "Total data transferred to and from each GPU is then $\\frac{2 (N - 1) K}{N}$\n",
    "\n",
    "For large $N$, this is independent of $N$, which allows the algorithm to scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General Computation*\n",
    "\n",
    "We looked at illustration with elementwise addition\n",
    "\n",
    "Also applies for more complex calcs such as gradient aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### IV. Asynchronous Training\n",
    "\n",
    "We apply data parallelism here as well\n",
    "\n",
    "Goal: Allows workers to work independently (not waiting on other workers)\n",
    "\n",
    "Asynchronous training can be more efficient than synchronous training since there is no waiting. \n",
    "\n",
    "This is especially helpful when there is variation in the computing power across workers.\n",
    "\n",
    "One way to achieve this is by using a parameter server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous Algorithm Example: Parameter Server\n",
    "\n",
    "In this approach, weights and biases of ML model are distributed across nodes in cluster  \n",
    "\n",
    "A copy of the model is stored on each node and a centralized *parameter server* manages model changes.  \n",
    "This pattern is called *centralized training*.\n",
    "\n",
    "Here is how it works:\n",
    "\n",
    "1. Designate some nodes as parameter servers, while others train the model\n",
    "\n",
    "- Parameter servers store model parameters, update global state of model\n",
    "\n",
    "2. Training workers each get a subset of data\n",
    "3. Each training worker fetches parameters from parameter servers\n",
    "4. Each training worker performs training loop,  \n",
    "   calculates gradients and loss,  \n",
    "   sends gradients back to all parameter servers, which updates model parameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/parameter_server.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges with this approach**\n",
    "\n",
    "At any given time, only one worker is using updated version of model. Others are using stale version. \n",
    "\n",
    "If one worker is used as parameter server, it can be a bottleneck and single point of failure.  \n",
    "Using *multiple parameter servers* alleviates this issue but adds communication costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Parameter Server Example: Gorila - General Reinforcement Learning Architecture**\n",
    "\n",
    "*Source: Mastering Reinforcement Learning with Python, Enes Bilgin.*\n",
    "\n",
    "Background:  \n",
    "- Deep Reinforcement Learning (DRL) requires massive training datasets to train a neural network. There is no label used; instead, a reward function is specified.\n",
    "- The *Q-network* is one approach in DRL.\n",
    "- An agent exists in some state (e.g., an investor holds a portfolio of stocks) and must take a series of actions over time (buy/sell orders).\n",
    "- Everything outside of the agent is the environment (the economy, the stock markets)\n",
    "- We might have historical data (experiences) that can be used\n",
    "- The goal is to learn the best action to take given each state. This is the *optimal policy*. It needs to maximize long-term reward.\n",
    "\n",
    "Strategy: we need to simulate experiences, and we can parallelize this. We store experiences in a replay buffer.\n",
    "\n",
    "Here are the important components:\n",
    "\n",
    "**Actors:**  \n",
    "Processes that interact with copy of environment given a policy, take the action, observe the reward, and transition to the next state.\n",
    "\n",
    "Actors get a copy of the Q-network from a parameter server\n",
    "\n",
    "**Replay Buffer:**  \n",
    "Actors collect experiences and store them in a replay buffer. This can be done in a distributed fashion.\n",
    "\n",
    "**Learners:**  \n",
    "Learners calculate the gradients that update the Q-network in the parameter server.  \n",
    "A learner has a copy of the Q-network, it samples experiences from the buffer, calculates the loss and gradients, and sends them back to the parameter server\n",
    "\n",
    "**Parameter Server:**  \n",
    "The parameter server stores the main copy of the Q-network. As learning progresses, updates happen here.    \n",
    "All processes sync their version of the Q-network from here.\n",
    "\n",
    "--\n",
    "\n",
    "**Next, we provide an overview of Gorila**\n",
    "\n",
    "Gorila provides a general framework to parallelize deep Q-learning\n",
    "\n",
    "It consists of bundles which can be distributed to workers, where each bundle comprises an action, learner, and local replay buffer.  \n",
    "\n",
    "Next we show an illustration of the approach. For our purposes, the details are unimportant; the takeaway is how the parameter server supports learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/gorila.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the learners send gradients back to the parameter server\n",
    "\n",
    "The learners get the synced Q-network from the parameter server\n",
    "\n",
    "**Shortcoming:** There is a lot of passing around of parameters between the actors, learners, and parameter server.  \n",
    "This can be a significant communication load.\n",
    "\n",
    "If you are interested, you can [explore](https://arxiv.org/pdf/1803.00933) how the *Ape-X* architecture improves Gorila!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### V. Implementation\n",
    "\n",
    "**Elephas** is a Keras add-on that allows you to use Spark to execute distributed deep learning models at scale.\n",
    "\n",
    "**Horovod**. Open-source distributed training framework developed at Uber for TensorFlow, Keras, PyTorch, and MXNet.  \n",
    "Supports practical distributed training over several GPUs and nodes.  \n",
    "Documentation can be found [here](https://horovod.readthedocs.io/en/stable/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Conclusions\n",
    "\n",
    "We have studied different approaches for distributed model training\n",
    "\n",
    "Benefits can include scalability, efficiency, and fault tolerance \n",
    "\n",
    "These come at the cost of increased network communication load\n",
    "\n",
    "Data parallelism (partitioning the data across nodes) is easier than model parallelism\n",
    "\n",
    "Model parallelism is useful when the model won't fit on a single machine. \n",
    "\n",
    "Vertical partitioning is easier for deep learning, as layers are kept together on workers\n",
    "\n",
    "There are several packages that implement distributed model training\n",
    "\n",
    "**Tradeoffs**\n",
    "\n",
    "As always, there are tradeoffs to the methods:\n",
    "\n",
    "- Centralized training, such as the parameter server approach, can introduce a communication bottleneck. More parameter servers can be added to reduce the bottleneck.\n",
    "\n",
    "- The parameter server approach is asynchronous which can increase efficiency: there is no waiting for workers. This is especially helpful when the nodes have differences in processing power.\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
