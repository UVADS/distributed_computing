{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uva_seal.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib Clustering\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: September 29, 2024\n",
    "\n",
    "---  \n",
    "\n",
    "\n",
    "### SOURCES  \n",
    "- Learning Spark, Chapter 11: Machine Learning with MLlib  \n",
    "\n",
    "- https://spark.apache.org/docs/latest/ml-clustering.html\n",
    "\n",
    "- [Cluster cohesion](https://towardsdatascience.com/explain-ml-in-a-simple-way-k-means-clustering-e925d019743b)\n",
    "\n",
    "- [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n",
    "\n",
    "- [Silhouette toy example](https://medium.com/@MrBam44/how-to-evaluate-the-performance-of-clustering-algorithms-3ba29cad8c03)\n",
    "\n",
    "### OBJECTIVES\n",
    "Introduction to some of the major clustering techniques in MLlib using the DataFrame API\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Unsupervised learning\n",
    "- K-means\n",
    "- Silhouette Score\n",
    "- Mixture of Gaussians\n",
    "\n",
    "---\n",
    "\n",
    "**Unsupervised Learning**  \n",
    "In this task, labels are unknown and the analyst wishes to segment the observations into groups of high similarity, where similarity is defined in terms of the feature space.\n",
    "\n",
    "Common use cases are:\n",
    "- Data exploration to discover the properties of similar observations  \n",
    "- Outlier detection; outliers will generally form their own group (e.g., singletons)  \n",
    "\n",
    "**K-Means**  \n",
    "This is the most popular clustering algorithm, with widespread use in industry. It is relatively simple, uses a single parameter, and converges on a solution (but possibly not the global maximum).\n",
    "\n",
    "The following models are supported in `spark.mllib` with the DataFrame API:\n",
    "\n",
    "- K-means\n",
    "- Gaussian mixture\n",
    "- Power iteration clustering (PIC)\n",
    "- Latent Dirichlet allocation (LDA)\n",
    "- Bisecting k-means\n",
    "\n",
    "**<center>K-Means Specs</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Item   | Description |\n",
    "| -------- | ----------- |\n",
    "| Supervised/Unsupervised | Unsupervised |\n",
    "| Initialization | Random Assignment |\n",
    "| Assumptions | Euclidean Distance |\n",
    "| Preprocessing | Scaling |\n",
    "| Parameters | $K:$ number of clusters |\n",
    "| Metrics | Inertia |\n",
    "| Strengths | One parameter, relatively simple |\n",
    "| Weaknesses | 1. May not find global optimum <br> 2. Can't handle non-quant data (e.g., categorical)<br> 3. Assumes spherical cluster shape|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Sample 2D Visualization ($K=3$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"k_means_before_after.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| K-Means Sample Workflow | \n",
    "| -------- | \n",
    "| 1. feature selection | \n",
    "| 2. feature standardization | \n",
    "| 3. run algo for sequence of $K$ |  \n",
    "| 4. examine results and remediate outliers <br> <span style=\"color:red\">loop on 3-4 as needed</span>| \n",
    "| 5. select $K^*$, extract cluster assignments | \n",
    "| 6. enrich with domain knowledge | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Metric**\n",
    "\n",
    "To measure the quality of clustering, we use *within cluster sum of squares (WSS)*.  \n",
    "\n",
    "For each cluster, we compute sum of squared distance between each point and the centroid.  \n",
    "Then we compute these sums across all clusters. It is a measure of internal cohesion of clusters.  \n",
    "\n",
    "$$ \\texttt{Within Sum of Squares} \\,\\, (WSS) = 1 - \\frac{\\texttt{Between Sum of Squares}}{\\texttt{Total Sum of Squares}} $$\n",
    "\n",
    "This diagram illustrates:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cohesion.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means: Selecting $K^*$**\n",
    "\n",
    "Below we construct a scree plot of WSS versus number of clusters.  \n",
    "One method for selecting $K^*$ is by identifying the elbow in a scree plot.  At the inflection point, adding more clusters reduces WSS only marginally.  Generally, well-formed clusters are split apart, creating new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='scree_plot_k_means2.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means Implementation**\n",
    "\n",
    "`MLlib` contains an implementation of `K-means` and also `kmeans||`  \n",
    "`kmeans||` provides a better initialization in parallel environments.  \n",
    "\n",
    "Please see k-means extension [deck](https://github.com/UVADS/distributed_computing/blob/main/05_clustering_and_dim_reduction/content_clustering/k_means_extensions.ppt) for details.\n",
    "\n",
    "Included in the parameters,  \n",
    "initMode = 'random' or 'k-means||', where 'k-means||' is the default initialization method.\n",
    "\n",
    "\n",
    "**Methods:**  \n",
    "Train the model with `fit()`  \n",
    "Can access `clusterCenters` as an array of vectors  \n",
    "Can call `transform()` on a new vector to return its assigned cluster;   this is the closest center.\n",
    "\n",
    "**K-Means Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Loads data\n",
    "df = spark.read.csv(\"kmeans_data.txt\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats =  ['f1','f2','f3']      \n",
    "assembler = VectorAssembler(inputCols=feats, outputCol=\"features\")\n",
    "dataset=assembler.transform(df)\n",
    "dataset.select(\"*\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the features in the first observation are saved in sparse format (since all values are zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a k-means model with k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(2).setSeed(314).setMaxIter(10)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions\n",
    "\n",
    "note the `transform()` method does prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(dataset)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice the cluster centers are very intuitive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    "\n",
    "The silhouette score measures the consistency within clusters.  \n",
    "\n",
    "It falls in range [-1, 1] where\n",
    "- a value close to 1 means that points are consistent (this is a good clustering)\n",
    "- a value near 0 indicates overlapping clusters  \n",
    "- a negative value indicates observations assigned to incorrect clusters\n",
    "\n",
    "Here is a toy example computing the silhouette score for a single point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./silhouette.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the algorithm to compute the silhouette score: \n",
    "\n",
    "1) For each observation (point), compute A and B with these definitions:\n",
    "\n",
    "   A : The mean distance of each point to its neighbors within its cluster (called the *mean intra-cluster distance*).\n",
    " \n",
    "   B : the mean distance of each point to its next-closest cluster (called the *mean nearest-cluster distance*). The `min()` function in the illustration above determines the mean distance of the nearest cluster. \n",
    "\n",
    "   For well-assigned points, quantity *A* should be much smaller than quantity *B*\n",
    "\n",
    "2) Compute for each point *i* the quantity:  $ s(i) = (B - A) / max(A, B)$\n",
    "\n",
    "3) The mean is then computed over all s(i) to arrive at the silhoutte score.\n",
    "\n",
    "You can read more about the metric [here](https://en.wikipedia.org/wiki/Silhouette_(clustering))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "The *Gaussian Mixture Model* is a weighted combination of underlying Gaussian distributions, each with a fixed probability.  \n",
    "The *expectation-maximization algorithm* is used in `spark.mllib` to estimate the parameters.  \n",
    "There is a mean vector and a covariance matrix for each cluster.  \n",
    "\n",
    "**Fit Mixture of Two Gaussians**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "\n",
    "# reuse data from K-Means example above\n",
    "\n",
    "gmm = GaussianMixture().setK(2).setSeed(314)\n",
    "model = gmm.fit(dataset)\n",
    "\n",
    "print(\"Gaussians shown as a DataFrame: \")\n",
    "print(\"component mean vectors\")\n",
    "model.gaussiansDF.select(\"mean\").show(truncate=False)\n",
    "\n",
    "print(\"component covariance matrices\")\n",
    "model.gaussiansDF.select(\"cov\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the mean vectors are very close to the k-means centroids.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRY FOR YOURSELF (UNGRADED EXERCISES)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) **K-Means: try different initialization mode**  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Change the setting and observe if the results change. Hint: use setInitMode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **K-Means: Try different K**  \n",
    "i. Copy the k-means example in the cell below  \n",
    "ii. Rerun k-mean with different k and observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) We considered a scree plot above, where the within sum of squared errors is measured for various values of k.  \n",
    "Is it possible to reduce the within sum of squares to zero, and if so, how? and is this a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Answer: By setting k = n (the number of observations), each observation will be placed in its own cluster. This will drive the within sum of squares to zero. However, this doesn't provide useful information, as no clustering is taking place."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
